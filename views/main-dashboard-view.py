# views/main-dashboard-view.py
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# Vue unifi√©e : France / Grand Est ‚Äî Prophet only + onglets D√©partement & R√©gion
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

import json
from pathlib import Path
from typing import Optional

import numpy as np
import pandas as pd
import plotly.express as px
import streamlit as st

# Acc√®s aux modules de mod√®les Prophet
import sys
sys.path.append(str(Path(__file__).resolve().parents[1]))  # racine du repo
try:
    from models import app as app_regional
    from models import app_national
except Exception:
    app_regional = None
    app_national = None

# -----------------------------
# CONFIG
# -----------------------------
st.set_page_config(
    page_title="Thermom√®tre Grippal Pr√©dictif",
    page_icon="üå°Ô∏è",
    layout="wide"
)

NATIONAL_CSV = Path("data/clean-data/donnees_analytiques_france.csv")
REGIONAL_CSV = Path("data/clean-data/donnees_analytiques_grand_est.csv")

# -----------------------------
# HELPERS : sch√©ma & conversions
# -----------------------------
FALLBACK_CANDIDATES = {
    "code_departement": [
        "code_departement", "dep_code", "code_dep", "dep", "departement",
        "departement_code", "code", "num_dep", "code_insee", "insee_dep"
    ],
    "nom_departement": [
        "nom_departement", "nom", "departement_nom", "libelle_departement",
        "libelle", "dep_nom"
    ],
    "annee_semaine": [
        "annee_semaine", "semaine", "week", "year_week", "anneeSemaine",
        "yearweek", "YW", "yyyyww", "periode_semaine", "periode"
    ],
    "total_cas_semaine": [
        "total_cas_semaine", "total_cas", "cas_total", "cas", "nb_cas",
        "nombre_cas", "total", "sum_cas"
    ],
}

def _first_present(df: pd.DataFrame, names: list[str]) -> Optional[str]:
    for n in names:
        if n in df.columns:
            return n
    return None

def coerce_schema(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    # Remap direct vers le sch√©ma
    mapping = {}
    for target, candidates in FALLBACK_CANDIDATES.items():
        src = _first_present(df, candidates)
        if src and src != target:
            mapping[src] = target
    if mapping:
        df = df.rename(columns=mapping)

    # Si 'total_cas_semaine' absent, essayer de le cr√©er √† partir de colonnes proches
    if "total_cas_semaine" not in df.columns:
        urg_candidates = ["urgences", "nb_urgences", "passages_urgences", "urg", "er_visits",
                          "cas_urgences_semaine"]
        sos_candidates = ["sos_medecins", "nb_sos_medecins", "sos", "acts_sos", "sos_actes",
                          "cas_sos_medecins_semaine"]
        u = _first_present(df, urg_candidates)
        s = _first_present(df, sos_candidates)
        if u and s:
            df["total_cas_semaine"] = df[u].fillna(0).astype(float) + df[s].fillna(0).astype(float)
        elif u and "total_cas_semaine" not in df.columns:
            df["total_cas_semaine"] = df[u].fillna(0).astype(float)
        elif s and "total_cas_semaine" not in df.columns:
            df["total_cas_semaine"] = df[s].fillna(0).astype(float)

    # Normalisation des identifiants
    if "code_departement" in df.columns:
        df["code_departement"] = df["code_departement"].astype(str).str.upper().str.strip()
        mask_corse = df["code_departement"].str.contains("A|B")
        df.loc[~mask_corse, "code_departement"] = df.loc[~mask_corse, "code_departement"].str.zfill(2)
        df["code_departement"] = df["code_departement"].str.replace("0A", "2A").str.replace("0B", "2B")

    if "annee_semaine" in df.columns:
        df["annee_semaine"] = df["annee_semaine"].astype(str)

    return df

def week_to_datetime(week_str: str) -> Optional[pd.Timestamp]:
    try:
        s = str(week_str)
        if "-S" in s:
            year, sw = s.split("-S")
            return pd.to_datetime(f"{int(year)}-W{int(sw):02d}-1")
        if "W" in s:
            year, sw = s.split("W")
            return pd.to_datetime(f"{int(year)}-W{int(sw):02d}-1")
        if len(s) >= 6 and s[:4].isdigit():
            year, sw = int(s[:4]), int(s[4:])
            return pd.to_datetime(f"{year}-W{sw:02d}-1")
    except Exception:
        return None
    return None

def ensure_required_columns(df: pd.DataFrame) -> list[str]:
    required = ["code_departement", "nom_departement", "annee_semaine", "total_cas_semaine"]
    return [c for c in required if c not in df.columns]

# -----------------------------
# CHARGEMENT g√©n√©rique
# -----------------------------
@st.cache_data(show_spinner=False)
def load_csv(path: Path) -> pd.DataFrame:
    if not path.exists():
        return pd.DataFrame()
    try:
        df = pd.read_csv(path, sep=";")
    except Exception:
        df = pd.read_csv(path)
    return coerce_schema(df)

def load_geojson_from_models() -> Optional[dict]:
    # Essaye les helpers des modules existants
    for mod in (app_national, app_regional):
        if mod:
            try:
                gj = mod.get_geojson()
                if gj:
                    return gj
            except Exception:
                pass
    # Fallback local si pr√©sent
    geo_path = Path("data/geo/departements.geojson")
    if geo_path.exists():
        with open(geo_path, "r", encoding="utf-8") as f:
            return json.load(f)
    return None

# -----------------------------
# SIDEBAR
# -----------------------------
st.sidebar.title("‚öôÔ∏è Param√®tres")

vue = st.sidebar.radio(
    "Vue",
    options=("üá´üá∑ France m√©tropolitaine", "üü£ R√©gion Grand Est"),
    index=0,
    help="Bascule entre la vue nationale et la vue Grand Est."
)

st.sidebar.markdown("---")
default_csv = str(NATIONAL_CSV) if vue.startswith("üá´üá∑") else str(REGIONAL_CSV)
csv_path = st.sidebar.text_input("Chemin du CSV analytique", value=default_csv)
st.sidebar.caption("Par d√©faut : data/clean-data/donnees_analytiques_*.csv")

# -----------------------------
# PIPELINE PROPHET
# -----------------------------
def run_prophet(vue_label: str, csv_path_str: str):
    """
    Charge le CSV, entra√Æne Prophet par d√©partement, calcule le score, renvoie :
      - df_hist : historique complet (sch√©ma coerc√©)
      - df_disp : derni√®re semaine par d√©partement avec colonnes pr√©dictives
      - last_week_label : libell√© de la derni√®re semaine utilis√©e
    """
    df_hist = load_csv(Path(csv_path_str))
    if df_hist.empty:
        return df_hist, pd.DataFrame(), None

    try:
        if vue_label.startswith("üá´üá∑"):
            if not app_national:
                raise RuntimeError("Module app_national indisponible")
            df_loaded = app_national.charger_donnees(csv_path_str)  # g√®re zfill + Corse
            if df_loaded.empty:
                raise RuntimeError("Chargement national vide")
            df_pred = app_national.entrainer_et_predire(df_loaded)
            df_disp = app_national.calculer_score(df_pred)
        else:
            if not app_regional:
                raise RuntimeError("Module app (r√©gional) indisponible")
            df_loaded = app_regional.charger_donnees(csv_path_str)
            if df_loaded.empty:
                raise RuntimeError("Chargement r√©gional vide")
            df_pred = app_regional.entrainer_et_predire(df_loaded)
            df_disp = app_regional.calculer_score(df_pred)

        last_week_label = str(df_disp["annee_semaine"].iloc[0])
        return df_hist, df_disp, last_week_label

    except Exception as e:
        st.error(
            "Le moteur **Prophet** a rencontr√© une erreur et ne peut pas √™tre remplac√© par un fallback "
            "car le MVP a √©t√© retir√© √† votre demande.\n\n"
            f"**D√©tail**: {e}\n\n"
            "‚û°Ô∏è Sous **Windows**, si l‚Äôerreur mentionne `tbb.dll`, lancez `fix_tbb.bat` en administrateur puis relancez l‚Äôapplication."
        )
        return df_hist, pd.DataFrame(), None

df_full, df_display, last_week_label = run_prophet(vue, csv_path)

st.title("üå°Ô∏è Thermom√®tre Grippal Pr√©dictif ‚Äî Vue unifi√©e (Prophet)")

if df_full.empty or df_display.empty:
    st.stop()

missing = ensure_required_columns(df_full)
if missing:
    st.warning(f"Colonnes manquantes dans le CSV : {', '.join(missing)}")

geojson = load_geojson_from_models()

# -----------------------------
# TABS
# -----------------------------
tab1, tab2, tab3, tab4 = st.tabs(["üó∫Ô∏è Carte & KPIs", "üè• Analyse d√©partement", "üó∫Ô∏è Analyse r√©gion", "‚ÑπÔ∏è √Ä propos du projet"])

# --- TAB 1 : CARTE & KPIs ---
with tab1:
    label_vue = "France m√©tropolitaine" if vue.startswith("üá´üá∑") else "R√©gion Grand Est"
    st.subheader(f"Carte de la tension ‚Äî {label_vue} ‚Äî semaine √† venir (S+1)")
    if last_week_label:
        st.caption(f"Derni√®re semaine historique prise en compte : **{last_week_label}** ¬∑ Moteur : **PROPHET**")

    # KPIs globaux
    k1, k2, k3 = st.columns(3)
    k1.metric("Score moyen", f"{df_display['score_global_predictif'].mean():.2f}")
    k2.metric("Cas pr√©dits (S+1)", f"{int(np.nan_to_num(df_display['cas_predits_semaine_suivante']).sum())}")
    k3.metric("Tendance moyenne", f"{np.nanmean(df_display['tendance_evolution_cas']):+.1%}")

    # Carte
    if geojson is None:
        st.warning("Fond de carte GeoJSON introuvable. Fournissez `data/geo/departements.geojson` (cl√© `feature.properties.code`).")
    else:
        if vue.startswith("üá´üá∑"):
            zoom = 4.6; center = {"lat": 46.6, "lon": 2.4}
        else:
            zoom = 6.5; center = {"lat": 48.6921, "lon": 6.1844}

        map_df = df_display.dropna(subset=["score_global_predictif"]).copy()

        fig = px.choropleth_mapbox(
            map_df,
            geojson=geojson,
            locations="code_departement",
            featureidkey="properties.code",
            color="score_global_predictif",
            color_continuous_scale=["#D6F5D6", "#FFF3B0", "#FFA500", "#FF5E5E", "#9E0031"],
            range_color=(0, 1),
            mapbox_style="carto-positron",
            zoom=zoom,
            center=center,
            opacity=0.75,
            hover_name="nom_departement",
            hover_data={
                "code_departement": False,
                "cas_predits_semaine_suivante": True,
                "tendance_evolution_cas": ":.1%",
                "score_global_predictif": ":.2f",
            },
        )
        fig.update_layout(margin={"r":0, "t":0, "l":0, "b":0})
        st.plotly_chart(fig, use_container_width=True)

    with st.expander("Comment lire la carte ?"):
        st.markdown(
            """
            **Couleur = score de tension pr√©dictif** (0 ‚Üí 1). Plus c'est **rouge**, plus le risque est √©lev√© pour S+1.  
            L‚Äôinfo-bulle montre **cas pr√©dits S+1**, **tendance S/S-1** et **score** exact.  
            Le moteur utilis√© est **Prophet** (saisonnalit√©s). 
            """
        )

# --- TAB 2 : ANALYSE D√âPARTEMENT ---
with tab2:
    st.subheader("Analyse d√©taill√©e par d√©partement")
    dep_options = sorted(df_display["nom_departement"].dropna().unique().tolist())
    dep_name = st.selectbox("Choisir un d√©partement", dep_options)

    dep_code = df_display.loc[df_display["nom_departement"] == dep_name, "code_departement"].iloc[0]

    df_hist_dep = df_full[df_full["code_departement"] == dep_code].copy()
    df_hist_dep["_week_date"] = df_hist_dep["annee_semaine"].apply(week_to_datetime)
    df_hist_dep = df_hist_dep.sort_values("_week_date")

    # KPIs d√©partement
    row = df_display[df_display["code_departement"] == dep_code].iloc[0]
    k1, k2, k3 = st.columns(3)
    k1.metric("Score de tension", f"{row.get('score_global_predictif', np.nan):.2f}")
    k2.metric("Cas pr√©dits (S+1)", f"{int(np.nan_to_num(row.get('cas_predits_semaine_suivante', 0)))}")
    k3.metric("Tendance des cas", f"{row.get('tendance_evolution_cas', 0.0):+.1%}")

    st.markdown("**Historique hebdomadaire des cas (urgences + SOS M√©decins)**")
    if not df_hist_dep["_week_date"].isna().all():
        fig_ts = px.line(
            df_hist_dep,
            x="_week_date",
            y="total_cas_semaine",
            markers=True,
            labels={"_week_date": "Semaine", "total_cas_semaine": "Total cas/semaine"},
            title=f"√âvolution des cas ‚Äî {dep_name}",
        )
        st.plotly_chart(fig_ts, use_container_width=True)
    else:
        st.info("Impossible de tracer l'historique (format 'annee_semaine' non reconnu).")

    st.download_button(
        label="‚¨áÔ∏è Exporter les donn√©es du d√©partement (CSV)",
        data=df_hist_dep.drop(columns=["_week_date"], errors="ignore").to_csv(index=False).encode("utf-8"),
        file_name=f"grippe_{dep_code}.csv",
        mime="text/csv",
    )

# --- TAB 3 : ANALYSE R√âGION ---
with tab3:
    st.subheader("Analyse r√©gionale")
    # On ne force pas un mapping en dur : on exploite les colonnes si elles existent.
    region_col = "nom_region" if "nom_region" in df_full.columns else ("code_region" if "code_region" in df_full.columns else None)

    if region_col is None:
        st.warning(
            "Aucune colonne r√©gion d√©tect√©e. Pour activer cette vue, ajoutez `nom_region` ou `code_region` "
            "dans le CSV analytique (ex. lors de la pr√©paration des donn√©es)."
        )
    else:
        regions = sorted(df_full[region_col].dropna().unique().tolist())
        if not regions:
            st.info("Aucune r√©gion d√©tect√©e dans les donn√©es.")
        else:
            default_region = "Grand Est" if "Grand Est" in regions else regions[0]
            selected_region = st.selectbox("S√©lectionner une r√©gion", options=regions, index=regions.index(default_region))

            # Historique agr√©g√© r√©gion (somme des cas)
            reg_hist = df_full[df_full[region_col] == selected_region].copy()
            reg_hist["_week_date"] = reg_hist["annee_semaine"].apply(week_to_datetime)
            reg_hist = reg_hist.dropna(subset=["_week_date"]).sort_values("_week_date")
            agg = (
                reg_hist.groupby("_week_date", as_index=False)["total_cas_semaine"]
                .sum()
                .rename(columns={"total_cas_semaine": "total_cas_region"})
            )

            # KPIs r√©gion actuels (depuis df_display, somme/synth√®se)
            reg_disp = df_display.merge(
                df_full[[region_col, "code_departement"]].drop_duplicates(),
                on="code_departement", how="left"
            )
            reg_disp = reg_disp[reg_disp[region_col] == selected_region].copy()

            c1, c2, c3 = st.columns(3)
            c1.metric("D√©partements couverts", f"{reg_disp['code_departement'].nunique()}")
            c2.metric("Cas pr√©dits (S+1)", f"{int(np.nan_to_num(reg_disp['cas_predits_semaine_suivante']).sum())}")
            c3.metric("Score moyen", f"{reg_disp['score_global_predictif'].mean():.2f}")

            # Courbe r√©gionale
            st.markdown("**Historique agr√©g√© de la r√©gion (cas/semaine)**")
            if not agg.empty:
                fig_r = px.line(
                    agg,
                    x="_week_date",
                    y="total_cas_region",
                    markers=True,
                    labels={"_week_date": "Semaine", "total_cas_region": "Cas r√©gion (somme)"},
                    title=f"√âvolution des cas ‚Äî {selected_region}",
                )
                st.plotly_chart(fig_r, use_container_width=True)
            else:
                st.info("Pas assez de donn√©es pour tracer l'historique r√©gional.")

            # Tableau des d√©partements de la r√©gion, tri par score
            st.markdown("**D√©partements (tri√©s par score d√©croissant)**")
            table_cols = [
                "code_departement", "nom_departement",
                "cas_predits_semaine_suivante", "tendance_evolution_cas", "score_global_predictif"
            ]
            table = (
                reg_disp[table_cols]
                .sort_values("score_global_predictif", ascending=False)
                .reset_index(drop=True)
            )
            st.dataframe(table, use_container_width=True)

# --- TAB 4 : √Ä PROPOS DU PROJET ---
with tab4:
    st.header("√Ä Propos du Projet")
    st.markdown("""
Ce projet a √©t√© r√©alis√© dans le cadre du **Hackathon Sant√© Datalab x EPITECH**.

### Objectif
D√©velopper un outil d‚Äôaide √† la d√©cision pour optimiser la strat√©gie vaccinale contre la grippe en **anticipant les zones de tension** sur le syst√®me de sant√©.

### M√©thodologie (Prophet)
1. **Pr√©paration des donn√©es** : consolidation hebdomadaire par d√©partement (urgences + SOS M√©decins), standardisation des identifiants (codes INSEE), et enrichissements (couverture vaccinale, d√©mographie‚Ä¶).
2. **Mod√©lisation pr√©dictive** : **Prophet** par d√©partement (saisonnalit√©s annuelle/hebdomadaire si pertinent). La derni√®re pr√©diction **S+1** est consolid√©e pour l‚Äôaffichage.
3. **Scoring de tension** : combinaison pond√©r√©e
   - **Cas pr√©dits (S+1)** ‚Üí normalis√©s
   - **Vuln√©rabilit√© vaccinale** ‚Üí `1 - couverture`
   Le score final est born√© en [0, 1].

### Utilisation
- **Vue** : *France m√©tropolitaine* ou *Grand Est*
- **Carte** : score par d√©partement (plus rouge = plus de tension attendue).
- **Analyse D√©partement** : historique local + KPIs.
- **Analyse R√©gion** : KPIs, courbe agr√©g√©e, et classement des d√©partements (si `nom_region`/`code_region` est pr√©sent dans les donn√©es).

### Notes techniques
- Si vous √™tes sous **Windows** et rencontrez une erreur li√©e √† **`tbb.dll`**, utilisez `fix_tbb.bat` (ex√©cution **administrateur**), puis relancez l‚Äôapp.
- CSV attendu (min) : `code_departement`, `nom_departement`, `annee_semaine`, `total_cas_semaine`.  
  Colonnes utilis√©es si pr√©sentes : `cas_predits_semaine_suivante`, `tendance_evolution_cas`, `score_global_predictif`, `nom_region`/`code_region`.

---
    """)
